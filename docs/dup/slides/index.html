<!DOCTYPE html>
<html lang="en">
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="repo" content="https://github.com/gvwilson/sd4ds">
  <meta name="build_date" content="2023-05-30">
  <meta name="template" content="slides">
  <meta name="major" content="Chapter 2">
  
  <link rel="icon" type="image/x-icon" href="../../favicon.ico">
  <link rel="stylesheet" href="../../mccole.css">
  <link rel="stylesheet" href="../../tango.css">
  <script defer data-domain="third-bit.com" src="https://plausible.io/js/plausible.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']]
      }
    };
  </script>
  <script
    type="text/javascript"
    id="MathJax-script"
    async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script defer src="../../mccole.js"></script>
  <title>Software Design for Data Scientists: Finding Duplicate Files</title>
</head>

  <body>
    <textarea id="source">

class: slide-title

<p>
  <a href="https://third-bit.com/sd4ds/">Software Design for Data Scientists</a>
</p>
<h1>Finding Duplicate Files</h1>
<div class="bottom">
  <a href="../">chapter</a>
</div>

---


## Overview

-   We want to find duplicate files, but can't rely on their names

-   Comparing pairs of files byte by byte is slow

    -   And gets slower per file as the number of files grows

-   Better approach:

    -   Calculate an identifier for each file that depends on its content

    -   Group files with the same identifier and compare those

-   Even better: calculate identifiers so that if two files have the same ID, they're guaranteed to have the same content

---

## Start Simple

-   Take filenames as command-line arguments

-   Generate and print a list of duplicate pairs

```py
def find_duplicates(filenames):
    matches = []
    for left in filenames:
        for right in filenames:
            if same_bytes(left, right):
                matches.append((left, right))
    return matches


if __name__ == "__main__":
    duplicates = find_duplicates(sys.argv[1:])
    for (left, right) in duplicates:
        print(left, right)
```


---

## Byte by Byte

```py
def same_bytes(left_name, right_name):
    left_bytes = open(left_name, "rb").read()
    right_bytes = open(right_name, "rb").read()
    return left_bytes == right_bytes
```


-   `open(filename, "r")` opens a file for reading characters

-   But images, audio clips, etc. aren't character data

-   So use <code>open(filename, "r<strong>b</strong>")</code> to open in <a class="gl-ref" href="../../glossary/#binary_mode" markdown="1">binary mode</a>

    -   Look at the difference in more detail in <a class="x-ref" href="../../binary/">Chapter 17</a>

---

## Test Case

-   Create a `tests` directory with six files

| `a1.txt` | `a2.txt` | `a3.txt` | `b1.txt` | `b2.txt` | `c1.txt` |
| ---- | ---- | ---- | ---- | ---- | ---- |
| aaa | aaa | aaa | bb | bb | c |

-   Expect the three `a` files and the two `b` files to be reported as duplicates

-   No particular reason for these tests—we just have to start somewhere

---

## Test Output

```sh
python brute_force_1.py tests/*.txt
```


```
tests/a1.txt tests/a1.txt
tests/a1.txt tests/a2.txt
tests/a1.txt tests/a3.txt
tests/a2.txt tests/a1.txt
tests/a2.txt tests/a2.txt
tests/a2.txt tests/a3.txt
tests/a3.txt tests/a1.txt
tests/a3.txt tests/a2.txt
tests/a3.txt tests/a3.txt
tests/b1.txt tests/b1.txt
tests/b1.txt tests/b2.txt
tests/b2.txt tests/b1.txt
tests/b2.txt tests/b2.txt
tests/c1.txt tests/c1.txt
```


-   Oops

---

## Revise Our Approach

```py
def find_duplicates(filenames):
    matches = []
    for i_left in range(len(filenames)):
        left = filenames[i_left]
        for i_right in range(i_left):
            right = filenames[i_right]
            if same_bytes(left, right):
                matches.append((left, right))
    return matches
```


<figure>
<img src="../triangle.svg" alt="Looping over distinct combinations"/>
</figure>


---

## Algorithmic Complexity

-   \\( N \\) objects can be paired in \\( N(N-1)/2 \\) ways

-   So for very large \\( N \\), work is proportional to \\( N^2 \\)

-   Computer scientist would say "<a class="gl-ref" href="../../glossary/#time_complexity" markdown="1">time complexity</a> is \\( O(N^2) \\)"

    -   Pronounced "<a class="gl-ref" href="../../glossary/#big_oh" markdown="1">big-oh</a> of N squared"

-   In practice, this means that the time per file increases as the number of files increases

-   Sometimes unavoidable, but in this case there's a better way

---

## Grouping Files

-   Process each file once to produce a short identifier

-   I.e., use a <a class="gl-ref" href="../../glossary/#hash_function" markdown="1">hash function</a> to produce a <a class="gl-ref" href="../../glossary/#hash_code" markdown="1">hash code</a>

-   Only compare files with the same identifier

<figure>
<img src="../hash_group.svg" alt="Grouping by hash code"/>
</figure>


---

## Naive Hashing

-   Bytes are just numbers

```py
def naive_hash(data):
    return sum(data) % 13
```


```py
    example = bytes("hashing", "utf-8")
    for i in range(1, len(example) + 1):
        substring = example[:i]
        hash = naive_hash(substring)
        print(f"{hash:2} {substring}")
```


```
 0 b'h'
 6 b'ha'
 4 b'has'
 4 b'hash'
 5 b'hashi'
11 b'hashin'
10 b'hashing'
```


---

## How Good Is This?

-   Want all hash codes to be equally likely

    -   Large file groups are disproportionately expensive

-   Hash each line of the novel *Dracula* and plot distribution

<figure>
<img src="../naive_dracula.svg" alt="Hash codes of <em>Dracula</em>"/>
</figure>


---

## After a Little Digging…

-   Our text file uses a blank line to separate paragraphs

-   So it's no surprise that 0 is the most common hash code

-   Look at the distribution of hash codes of *unique* lines

<figure>
<img src="../naive_dracula_unique.svg" alt="Hash codes of unique lines of <em>Dracula</em>"/>
</figure>


---

## Modifying Our Program

-   Build a dictionary with hash codes as keys and sets of files as values

```py
def find_groups(filenames):
    groups = {}
    for fn in filenames:
        data = open(fn, "rb").read()
        hash_code = naive_hash(data)
        if hash_code not in groups:
            groups[hash_code] = set()
        groups[hash_code].add(fn)
    return groups
```


-   If we haven't seen this key before, add it with an empty value

---

## Modifying Our Program

-   We can re-use most of the previous code

```py
    groups = find_groups(sys.argv[1:])
    for filenames in groups.values():
        duplicates = find_duplicates(list(filenames))
        for (left, right) in duplicates:
            print(left, right)
```


```
tests/a1.txt tests/a2.txt
tests/a3.txt tests/a2.txt
tests/a3.txt tests/a1.txt
tests/b1.txt tests/b2.txt
```


---

## But We Can Do Better

-   Use a <a class="gl-ref" href="../../glossary/#cryptographic_hash_function" markdown="1">cryptographic hash function</a>

    -   Output is completely deterministic

    -   But also unpredictable

    -   And distributed like a uniform random variable

-   Output depends on *order* of input as well as *value*

    -   With overwhelming probability, any change in input will produce a different output

---

## We Don't Need Groups

-   Odds that two people don't share a birthday are \\( 364/365 \\)

-   Odds that three people don't have are \\( (364/365) {\times} (363/365) \\)

-   There's a 50% chance of two people sharing a birthday in a group of 23 people
    and a 99.9% chance with 70 people

-   How many files do we need to hash before there's a 50% chance of a <a class="gl-ref" href="../../glossary/#collision" markdown="1">collision</a>
    with a 256-bit hash?

-   Answer is "approximately \\( 4{\times}10^{38} \\) files"

-   We're willing to take that risk

---

## SHA256 Example

```py
    example = bytes("hash", "utf-8")
    for i in range(1, len(example) + 1):
        substring = example[:i]
        hash = sha256(substring).hexdigest()
        print(f"{substring}\n{hash}")
```


```
b'h'
aaa9402664f1a41f40ebbc52c9993eb66aeb366602958fdfaa283b71e64db123
b'ha'
8693873cd8f8a2d9c7c596477180f851e525f4eaf55a4f637b445cb442a5e340
b'has'
9150c74c5f92d51a92857f4b9678105ba5a676d308339a353b20bd38cd669ce7
b'hash'
d04b98f48e8f8bcc15c6ae5ac050801cd6dcfd428fb5f9e65c4e16e7807340fa
```


-   `hexdigest` gives <a class="gl-ref" href="../../glossary/#hexadecimal" markdown="1">hexadecimal</a> representation of 256-bit hash code

---

## Duplicate Finder

```py
import sys
from hashlib import sha256

def find_groups(filenames):
    groups = {}
    for fn in filenames:
        data = open(fn, "rb").read()
        hash_code = sha256(data).hexdigest()
        if hash_code not in groups:
            groups[hash_code] = set()
        groups[hash_code].add(fn)
    return groups


if __name__ == "__main__":
    groups = find_groups(sys.argv[1:])
    for filenames in groups.values():
        print(", ".join(sorted(filenames)))
```


---

## Duplicate Finder

```sh
python dup.py tests/*.txt
```


```
tests/a1.txt, tests/a2.txt, tests/a3.txt
tests/b1.txt, tests/b2.txt
tests/c1.txt
```


-   Runtime is \\(O(N)\\), i.e., fixed time per file

-   Which is as good as it can possibly be

---

class: aside

## Learning Debt

-   What else can we use hashing for?

    -   Dictionaries

    -   Version control

-   How can we test our duplicate finder?

-   How can we make sure the code follows style guidelines?

-   How can we package it for others to use?

---

class: summary

## Summary

<figure>
<img src="../concept_map.svg" alt="Concept map for finding duplicate files"/>
</figure>



[academic_prototyping]: https://www.fuzzingbook.org/html/AcademicPrototyping.html
[aosa]: https://aosabook.org/
[beautiful_soup]: https://beautiful-soup-4.readthedocs.io/
[birthday_problem]: https://en.wikipedia.org/wiki/Birthday_problem
[black]: https://black.readthedocs.io/
[book_repo]: https://github.com/gvwilson/sd4ds/
[book_site]: https://third-bit.com/sd4ds/
[browser_engine_tutorial]: https://limpet.net/mbrubeck/2014/08/08/toy-layout-engine-1.html
[browser_engineering]: https://browser.engineering/
[brubeck_matt]: https://limpet.net/mbrubeck/
[cc_by_nc]: https://creativecommons.org/licenses/by-nc/4.0/
[cc_by_nc_legal]: https://creativecommons.org/licenses/by-nc/4.0/legalcode
[clarkes_laws]: https://en.wikipedia.org/wiki/Clarke%27s_three_laws
[contributor_covenant]: https://www.contributor-covenant.org/
[cook_mary_rose]: https://maryrosecook.com/
[crafting_interpreters]: https://craftinginterpreters.com/
[ctan]: https://www.ctan.org/
[cypress]: https://www.cypress.io/
[db_tutorial]: https://cstack.github.io/db_tutorial/
[dresser_christopher]: https://en.wikipedia.org/wiki/Christopher_Dresser
[ejs]: https://ejs.co/
[email]: mailto:gvwilson@third-bit.com
[eniac_programmers]: http://eniacprogrammers.org/
[ethical_source]: https://ethicalsource.dev
[evans_julia]: https://jvns.ca/
[evans_zines]: https://wizardzines.com/
[expect]: https://en.wikipedia.org/wiki/Expect
[flake8]: https://flake8.pycqa.org/
[git]: https://git-scm.com/
[git_man_page_generator]: https://git-man-page-generator.lokaltog.net/
[gitlet]: http://gitlet.maryrosecook.com/
[glosario]: https://github.com/carpentries/glosario
[gnu_make]: https://www.gnu.org/software/make/
[gutenberg]: https://www.gutenberg.org/
[harrelson_chris]: https://twitter.com/chrishtr
[hippocratic_license]: https://firstdonoharm.dev/
[hoye_mike]: http://exple.tive.org/blarg/
[html5_data_attributes]: https://developer.mozilla.org/en-US/docs/Learn/HTML/Howto/Use_data_attributes
[human_resource_machine]: https://tomorrowcorporation.com/humanresourcemachine
[isort]: https://pycqa.github.io/isort/
[ivy]: https://www.dmulholl.com/docs/ivy/main/
[jekyll]: https://jekyllrb.com/
[kilo_editor]: https://viewsourcecode.org/snaptoken/kilo/index.html
[latex]: https://www.latex-project.org/
[loewy_raymond]: https://en.wikipedia.org/wiki/Raymond_Loewy
[lorgat_editor]: https://github.com/seem/editor
[lorgat_tutorial]: https://wasimlorgat.com/posts/editor.html
[lorgat_wasim]: https://wasimlorgat.com/
[marthas_rules]: https://journals.sagepub.com/doi/10.1177/088610998600100206
[mkdocs]: https://www.mkdocs.org/
[nison_mael]: https://arcanis.github.io/
[nystrom_bob]: http://journal.stuffwithstuff.com/
[package_manager_tutorial]: https://classic.yarnpkg.com/blog/2017/07/11/lets-dev-a-package-manager/
[panchekha_pavel]: https://pavpanchekha.com/
[pexpect]: https://pexpect.readthedocs.io/
[php]: https://www.php.net/
[picosat]: http://fmv.jku.at/picosat/
[pip]: https://pip.pypa.io/
[programming_tools]: https://en.wikipedia.org/wiki/Programming_tool
[punching_holes]: http://exple.tive.org/blarg/2020/11/26/punching-holes/
[py_array]: https://docs.python.org/3/library/array.html
[py_ast]: https://docs.python.org/3/library/ast.html
[py_chainmap]: https://docs.python.org/3/library/collections.html#collections.ChainMap
[py_cprofile]: https://docs.python.org/3/library/profile.html
[py_glob]: https://docs.python.org/3/library/glob.html
[py_hashlib]: https://docs.python.org/3/library/hashlib.html
[py_inspect]: https://docs.python.org/3/library/inspect.html
[py_io]: https://docs.python.org/3/library/io.html
[py_itertools]: https://docs.python.org/3/library/itertools.html
[py_json]: https://docs.python.org/3/library/json.html
[py_metaclass]: https://docs.python.org/3/reference/datamodel.html#metaclasses
[py_mimetypes]: https://docs.python.org/3/library/mimetypes.html
[py_pickle]: https://docs.python.org/3/library/pickle.html
[py_property]: https://docs.python.org/3/library/functions.html#property
[py_semver]: https://pypi.org/project/semantic-version/
[py_textwrap]: https://docs.python.org/3/library/textwrap.html
[py_urlparse]: https://docs.python.org/3/library/urllib.parse.html
[pyfakefs]: https://pytest-pyfakefs.readthedocs.io/
[pytest]: https://docs.pytest.org/
[python]: https://www.python.org/
[reim_michael]: https://elderlinux.org/
[requests]: https://requests.readthedocs.io/
[ruten_paige]: https://viewsourcecode.org/
[selenium]: https://www.selenium.dev/
[semver_spec]: https://semver.org/
[sinel_joseph]: https://en.wikipedia.org/wiki/Joseph_Claude_Sinel
[sphinx]: https://www.sphinx-doc.org/
[sqlite]: https://sqlite.org/
[stack_connor]: https://connorstack.com/
[svg_screenshot]: https://chrome.google.com/webstore/detail/svg-screenshot/nfakpcpmhhilkdpphcjgnokknpbpdllg
[t3_personas]: https://teachtogether.tech/en/index.html#s:process-personas
[udhr]: https://www.un.org/en/universal-declaration-human-rights/
[ungc]: https://www.unglobalcompact.org/what-is-gc/mission/principles
[unix_glob]: https://en.wikipedia.org/wiki/Glob_(programming)
[unix_packaging]: https://eerielinux.wordpress.com/2017/08/15/the-history-of-nix-package-management/
[webaim_wave]: https://wave.webaim.org/
[write_yourself_a_git]: https://wyag.thb.lt/
[z3]: https://en.wikipedia.org/wiki/Z3_Theorem_Prover
[zeller_andreas]: https://andreas-zeller.info/

    </textarea>
    <script src="../../remark-latest.min.js"></script>
    <script>
      var slideshow = remark.create({
        highlightLines: true
      })
    </script>
  </body>
</html>
